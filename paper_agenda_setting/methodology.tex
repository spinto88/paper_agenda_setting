
\section{Methodology} \label{sec:Methodology}

\subsection{Numerical representation of the corpus}

\par In order to perform the analysis of the articles in the corpus, we describe them as numerical vectors through the \textit{term frequency - inverse document frequency (tf-idf)} representation. \textit{Tf-idf} gives greater values to terms that appear in less documents of the corpus (i.e more specific terms) and/or to those which appear more frequent in a document.

\par Given the set of terms made up by all the corpus' words after removing the non-informative ones, such as prepositions and conjunctions, the \textit{tf-idf} algorithm represents the \textit{i}-document as a vector $v_i = [x_{i1}, x_{i2}, ... , x_{it}]$, where the component $x_{ij}$ is computed by the equation \ref{ec:tfidf}, where $\textrm{tf}_{ij}$ is the number of times the \textit{j}-term appears in the \textit{i}-document, $d$ is the number of documents in the corpus, and $n_j$ is the number of documents where the \textit{j}-term appears. Each document's vector is normalized to unit Euclidean length.
Once the documents' vector are constructed, we join them in a document-term matrix (\emph{M}), which has dimensions of number of documents in the corpus ($d$) per number of terms selected ($t$).

% See parameters of sklearn.feature_extraction.text.TfidfVectorizer
\begin{center}
\begin{equation}
\begin{split}
\text{idf}_{j} & = 1 + \textrm{log}(\frac{1 + d}{1 + n_j}) \\
x_{ij} = \textrm{tf}_{ij} \cdot \textrm{idf}_{j}
\end{split}
\label{ec:tfidf}
\end{equation}
\end{center}

\subsection{Topic detection}
\par We perform \emph{non-negative matrix factorization (NMF)} on the document-term matrix (\emph{M}) in order to detect the main topics in the corpus. 
\emph{NMF} is an algorithm which factorize a matrix \emph{M} into two matrices \emph{W} and \emph{H} (eq.\ref{eq:nmf}), with the property that all three matrices have no negative elements. This non-negativity makes the resulting matrices easier to inspect, and very suitable for topic detection.
\begin{equation}
M^{(d \times t)} \sim H^{(d \times k)} \cdot W^{(k \times t)}
\label{eq:nmf}
\end{equation}
\par As can be seen in eq.\ref{eq:nmf}, the resulting matrix \emph{H} has dimensions of number of documents per $k$, and matrix \emph{W} has dimensions of $k$ per number of terms. The number $k$ is interpreted as the number of topics in the documents and it is a parameter that must be set before the factorization.
The matrix \emph{H} is interpreted as the representation of the documents in the topic-space, and the matrix \emph{W} as the topics represented in the original term-space. 
\par This factorization usually can not be made exactly so it is approximated by minimizing the reconstruction error, i.e. the distance between matrix \emph{M} and its approximated form $\tilde{M} = H \cdot W$. We performed \emph{NMF} through the python module \emph{scikit-learn} \cite{scikit-learn}. % See parameters of sklearn.decomposition.NMF

\subsection{Topic interpretation and temporal profiles}
\par The matrix $H$ obtained by \textit{NMF} gives the representation of the documents in the topic-space. In order to improve its interpretation we normalize each document's vector described in that space to unit $l_1$-norm. Therefore the components of these vectors can be viewed as a degree of membership of a given document in the set of topics. The index of the largest component tells us which is the most representative topic of the document.
\par On the other hand, each row of matrix $W$ represent the topic over the term-space. Therefore, the terms associated with the largest components of the \textit{i}-row are the most representative ones and give an insight of what the topic is talking about.
\par We define the temporal profile of the topic $i$, $W_i(day)$ by the eq. \ref{eq:topic_weight} where $l(j)$ is the number of words of the document $j$, and $h_{ji}$ is the degree of membership of document $j$ on topic $i$. This definition allows all documents to contribute to any topic weight, providing by the fact that each document's vector can have non-zero components in more than one topic.
\par As last steps of the data's preprocessing, we filter the topics' weight in order to reduce the noise but keeping the most details as possible, by redefining $W_i(day)$ as the mean value of a 3 days width window, centered on the day, as described in equation \ref{eq:topic_weight_norm}. 
\par Finally we normalize again all the temporal profiles in order to describe each newspaper as dsitribution over the topics' space which evolves over time. This last normalization prevent us against the differences in the number of articles that each newspaper publishes.

\begin{equation}
W_i(day) = \sum_j^d l(j) \cdot h_{ji} \cdot \delta_{d,day}
\label{eq:topic_weight}
\end{equation}

\begin{equation}
\tilde{W}_i(day) = \frac{1}{3} ({W}_i(day) + {W}_i(day - 1) + W_i(day + 1))
\label{eq:topic_weight_norm}
\end{equation}


\subsection{Outliers identification}
\label{sec:outliers_identification}

\par We identify outliers values in a data set of N observations by following the box plot construction. The quantities (called fences) in \ref{eq:fences} are used, where $Q1$ is the lower quartile (range of the distribution where lies the 25th percent of the data), $Q3$ is the upper quartile (where lies the 75th percent of the data) and $IQ = Q3 - Q1$.

\begin{equation}
\begin{split}
\text{lower inner fence (LIF)} & = Q1 - 1.5  \text{IQ} \\
\text{upper inner fence (UIF)} & = Q3 + 1.5  \text{IQ} \\
\text{lower outer fence (LOF)} & = Q1 - 3  \text{IQ} \\
\text{upper outer fence (UOF)} & = Q3 + 3  \text{IQ}
\end{split}
\label{eq:fences}
\end{equation}

\par A point above the upper inner fence considered a mild outlier and a point aove an outer fence is considered an extreme outlier. The same holds for the lower fences. \footnote{http://www.itl.nist.gov/div898/handbook/prc/section1/prc16.htm}

\subsection{Jensen shannon divergence}

\par In probability theory and statistics, the Jensen–Shannon divergence is a method of measuring the similarity between two probability distributions. It is based on the Kullback–Leibler divergence ($D_{KL}$)\ref{eq:jensen_shannon_distance}, but have useful properties such as it is symmetric and it is always a finite value. The square root of the Jensen–Shannon divergence \ref{eq:jensen_shannon_distance} is a metric often referred to as Jensen-Shannon distance. \cite{fuglede2004jensen}

\begin{equation}
\begin{split}
D_{KL}(P||Q) = -\sum{P(i) log(\frac{Q(i)}{P(i)})} \\
\text{JS Divergence}(P||Q) = \frac{1}{2}[D_{KL}(P||M) + D_{KL}(Q||M)] \\
\text{JS Distance (JSD)} = \sqrt(\text{JS Divergence})
\end{split}
\label{eq:jensen_shannon_distance}
\end{equation}


\subsection{Normalized Shannon's H Information Entropy}

\par The normalized Shannon's entropy \ref{eq:shannon_entropy} as a way to measure how spread is a distribution, taking the maximun value where all outcomes are equally probable in the case of a discrete distribution.

\begin{equation}
H[p] = \frac{- \sum_{i = 1}^{N} p(x_i) * ln(p(x_i))}{ln(N)}
\label{eq:shannon_entropy}
\end{equation}

